{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"../../..\")\n",
    "import numpy as np\n",
    "import gym\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from aerobench.visualize import anim3d, plot\n",
    "from aerobench.examples.anim3d.run_fall import fall_simulate\n",
    "from aerobench.examples.anim3d.run_rise import rise_simulate\n",
    "from aerobench.examples.anim3d.run_straight import straight_simulate\n",
    "from aerobench.examples.anim3d.run_right_turn import right_turn_simulate\n",
    "from aerobench.examples.anim3d.run_left_turn import left_turn_simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting to the screen. To save a video, pass a command-line argument ending with '.mp4' or '.gif'.\n"
     ]
    }
   ],
   "source": [
    "if len(sys.argv) > 1 and (sys.argv[1].endswith('.mp4') or sys.argv[1].endswith('.gif')):\n",
    "    filename = sys.argv[1]\n",
    "    print(f\"saving result to '{filename}'\")\n",
    "else:\n",
    "    filename = ''\n",
    "    print(\"Plotting to the screen. To save a video, pass a command-line argument ending with '.mp4' or '.gif'.\")\n",
    "simulation_functions = [\n",
    "        fall_simulate,\n",
    "        left_turn_simulate,\n",
    "        right_turn_simulate,\n",
    "        rise_simulate,\n",
    "        straight_simulate\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_16env(gym.Env):\n",
    "    def __init__(self,init_x,init_y,init_z):\n",
    "        self.res={}\n",
    "        self.missile=[5000,5000,5000]\n",
    "        self.psi=[]\n",
    "        self.distance=0\n",
    "        self.count=0\n",
    "        self.x=init_x\n",
    "        self.y=init_y\n",
    "        self.z=init_z\n",
    "        self.low=np.array([0,0,0],dtype=np.float32)\n",
    "        self.high=np.array([np.inf,np.inf,np.inf],dtype=np.float32)\n",
    "        self.action_space=spaces.Discrete(5)\n",
    "        self.observation_space=spaces.Box(self.low,self.high,dtype=np.float32)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        missile=np.array(self.missile)\n",
    "        plane=np.array([self.x,self.y,self.z])\n",
    "        self.distance= np.linalg.norm(plane-missile)\n",
    "        if self.distance<1000:\n",
    "            reward=-100\n",
    "        elif self.distance<=5000:\n",
    "            reward=0\n",
    "        else:\n",
    "            reward=100\n",
    "        return reward\n",
    "    \n",
    "    def calculate_done(self):\n",
    "        self.count+=1\n",
    "        done=False\n",
    "        if self.distance>10000 or self.count>=20:\n",
    "            done=True\n",
    "        return done\n",
    "    \n",
    "    def step(self,action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\" % (action,type(action),)\n",
    "        select_simulation=simulation_functions[action]\n",
    "        self.res, init_extra, skip_override, _=select_simulation(filename,self.x,self.y,self.z,0 if self.count==0 else self.res['states'][-1][5],2000)\n",
    "        self.x,self.y,self.z=self.res['states'][-1][10],self.res['states'][-1][9],self.res['states'][-1][11]\n",
    "        next_state=(self.x,self.y,self.z)\n",
    "        reward=self.calculate_reward()\n",
    "        done=self.calculate_done()\n",
    "        return next_state,reward,done,self.res, init_extra, skip_override\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.x,self.y,self.z=5100,5100,5100\n",
    "        return (self.x,self.y,self.z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"经验回放池\"\"\"\n",
    "    def __init__(self,capacity):\n",
    "        self.buffer=collections.deque(maxlen=capacity)\n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        self.buffer.append((state,action,reward,next_state,done))\n",
    "    def sample(self,batch_size):\n",
    "        transitions=random.sample(self.buffer,batch_size)\n",
    "        state,action,reward,next_state,done=zip(*transitions)\n",
    "        return np.array(state),action,reward,np.array(next_state),done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self,state_dim,hidden_dim,action_dim):\n",
    "        super(Qnet,self).__init__()\n",
    "        self.fc1=torch.nn.Linear(state_dim,hidden_dim)\n",
    "        self.fc2=torch.nn.Linear(hidden_dim,action_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,state_dim,hidden_dim,action_dim,learning_rate,gamma,\n",
    "                 epsilon,target_update,device):\n",
    "        self.action_dim=action_dim\n",
    "        self.q_net=Qnet(state_dim,hidden_dim,self.action_dim).to(device)\n",
    "        self.target_q_net=Qnet(state_dim,hidden_dim,self.action_dim).to(device)\n",
    "        self.optimizer=torch.optim.Adam(self.q_net.parameters(),lr=learning_rate)\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.target_update=target_update\n",
    "        self.count=0\n",
    "        self.device=device\n",
    "\n",
    "    def take_action(self,state):\n",
    "        if np.random.random()<self.epsilon:\n",
    "            action=np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state=torch.tensor([state],dtype=torch.float).to(self.device)\n",
    "            action=self.q_net(state).argmax().item()\n",
    "        return action\n",
    "    \n",
    "    def update(self,transition_dict):\n",
    "        states=torch.tensor(transition_dict['states'],dtype=torch.float).to(\n",
    "            self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        q_values=self.q_net(states).gather(1,actions)#Q(s,a)\n",
    "        max_next_q_values=self.target_q_net(next_states).max(1)[0].view(-1,1)#max(1)按行找最大，返回(值，索引)，故取第一个元素\n",
    "        q_targets=rewards+self.gamma*max_next_q_values*(1-dones)\n",
    "        dqn_loss=torch.mean(F.mse_loss(q_values,q_targets))\n",
    "        self.optimizer.zero_grad()\n",
    "        dqn_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count%self.target_update==0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count+=1\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waypoint transition Waypoint 1 -> Waypoint 2 at time 0\n",
      "Waypoint transition Waypoint 2 -> Waypoint 3 at time 11.699999999999969\n",
      "Waypoint transition Waypoint 3 -> Done at time 27.93333333333388\n",
      "Waypoint simulation completed in 0.35 seconds (extended_states=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\matplotlib\\animation.py:1741: UserWarning: Can not start iterating the frames for the initial draw. This can be caused by passing in a 0 length sequence for *frames*.\n",
      "\n",
      "If you passed *frames* as a generator it may be exhausted due to a previous display or save.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    lr=2e-3\n",
    "    hidden_dim=128#隐藏层神经元个数\n",
    "    gamma=0.98\n",
    "    epsilon=0.01\n",
    "    target_update=10\n",
    "    buffer_size=10000\n",
    "    minimal_size=500\n",
    "    batch_size=64\n",
    "    init_x,init_y,init_z=5100,5100,5100\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    env=F_16env(init_x,init_y,init_z)\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    env.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    replay_buffer=ReplayBuffer(buffer_size)\n",
    "    state_dim=3\n",
    "    action_dim=5\n",
    "    accumulated_res = {\n",
    "        'status': [], 'times': [], 'states': [], 'modes': [],\n",
    "        'xd_list': [], 'ps_list': [], 'Nz_list': [], 'Ny_r_list': [], 'u_list': [], 'runtime': []\n",
    "    }\n",
    "    agent=DQN(state_dim,hidden_dim,action_dim,lr,gamma,epsilon,target_update,device)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action=agent.take_action(state)\n",
    "        next_state,reward,done,res, init_extra, skip_override=env.step(action)\n",
    "        replay_buffer.add(state,action,reward,next_state,done)\n",
    "        state=next_state\n",
    "        if replay_buffer.size()>minimal_size:\n",
    "            n_s,n_a,n_r,n_ns,n_done=replay_buffer.sample(batch_size)\n",
    "            transition_dict={\n",
    "                'states':n_s,\n",
    "                'actions':n_a,\n",
    "                'rewards':n_r,\n",
    "                'next_states':n_ns,\n",
    "                'dones':n_done\n",
    "            }\n",
    "            agent.update(transition_dict)\n",
    "            accumulated_res['status'] = res['status']\n",
    "            accumulated_res['times'].extend(res['times'])\n",
    "            accumulated_res['states'].append(res['states'])\n",
    "            accumulated_res['modes'].extend(res['modes'])\n",
    "            if 'xd_list' in res:\n",
    "                accumulated_res['xd_list'].extend(res['xd_list'])\n",
    "                accumulated_res['ps_list'].extend(res['ps_list'])\n",
    "                accumulated_res['Nz_list'].extend(res['Nz_list'])\n",
    "                accumulated_res['Ny_r_list'].extend(res['Ny_r_list'])\n",
    "                accumulated_res['u_list'].extend(res['u_list'])\n",
    "            accumulated_res['runtime'].append(res['runtime'])\n",
    "            accumulated_res['states'] = np.vstack(accumulated_res['states'])\n",
    "\n",
    "    anim3d.make_anim(accumulated_res, filename, f16_scale=70, viewsize=5000, viewsize_z=4000, trail_pts=np.inf,\n",
    "                     elev=27, azim=-107, skip_frames=skip_override,\n",
    "                     chase=True, fixed_floor=True, init_extra=init_extra)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
